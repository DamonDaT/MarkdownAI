### 【LLM 性能评测数据集】

***

> 随着 NLP 和 LLM 的快速发展，各个细分领域里的模型评测数据集层出不穷，这里挑重点的主流的记录下

***





### 【一】NLU 自然语言理解

***

> 自然语言理解，该领域的数据集最为常见



#### 【1.1】GLUE

***

> General Language Understanding Evaluation ([GLUE]()) Benchmark：用于对英文自然语言理解（NLU）任务进行训练，校验和测试的数据集



##### 【1.1.1】自然语言推理（NLI）

***

* **ax**（NLI）

  该数据集适用于自然语言理解中的推理（NLI）任务，可使用在 MultiNLI 上训练的模型来针对此数据集进行后续预测校验。

* **mnli /mnli_matched / mnli_mismatched**（蕴含，矛盾，中立）

  语料是众包方式对句子进行 **文本蕴含** 注释。语料给定一个 **前提句子** 和一个 假设句子，任务内容是预测前提句子和假设句子是否逻辑相容（蕴含，矛盾，中立）。前提句来自于十种不同的语料来源，包括转录的演讲、小说、官方报告等。

* **rte**（蕴含，不蕴含）

  从 RTE1、RTE2、RTE3、RTE5 等多个年度 **文本蕴含** 挑战赛数据集整合而来的，其语料来源是新闻和百科。作者将原来的三分类转换为二分类任务，合并了中立和矛盾，并将标签设置为不蕴含。

***



##### 【1.1.2】语义相似度（SS）

***

* **mrpc**（相似，不相似）

  语料来自于线上新闻，经过人工标注的双句，进行语义相似度（有关 / 无关）判断的数据集。

* **qqp**（相似，不相似）

  预料来自 Quora，给定一个前提句子和一个假设句子，判断这两个句子在语义上是否相似。

* **stsb**（得分 (从1到5) ）

  语料来源是新闻、视频、图片标题等提取的句子对集合，并进行了人工标注。每个句子对标注了相似度得分（从1到5）。

***



##### 【1.1.3】语法正确性（GC）

***

* **cola**（语法正确性）

  语料库由来自语言理论书籍和期刊文章的英语可接受性判断组成。每个示例都是一个单词序列，并标注是否是符合语法的英语句子。

***



##### 【1.1.4】问答逻辑（QA）

***

* **qnli**（问答逻辑）

  来自于 SQuAD 数据集，后者是一个问答数据集，整个段落中的一句话包含了该问题的答案，模型要求将该答案句子从问题中预测出来。本数据集的作者过滤掉了上述问答句子之间词汇重叠较低的句子对，然后将该问答任务转换为双句分类任务，句子中的一个是问题，另一个是答案，本数据集预测两个句子是否存在问答逻辑。

***



##### 【1.1.5】情感分类（SC）

***

* **sst2**（情感分类）

  来自于电影的评论信息，并人工标注了它们的情感倾向。该数据集用于做单句双标签的情感分析任务。

***



##### 【1.1.6】指代替换（RS）

***

* **wnli**（指代替换）

  WNLI 是一个自然语言推断任务数据集。用该数据集测试的模型读入有代词的句子，并从一个列表中正确地选择出该代词指代的对象。这些句子是手动构建的，以避免简单统计方式对此任务的有效性：每个代词都取决于单个单词或短语提供的上下文信息。为了将该问题转换为双句分类问题，作者将原句中模棱两可的代词以每个可能的指代替换并构造双句，该任务需要预测这些代词被替换的句子是否被原句子蕴含。作者提供了一个小的测试集，其语料是原始数据集中私下共享的小说部分的一些新的示例。训练集的两个分类语料是均衡的，但测试集不是均衡的（65%的蕴含语料）。另外，验证集是对抗性的：'假设句子'有时候同时出现在训练和验证集中，因此一个模型如果记住了训练语料，它有可能在校验语料中预测出一个错误的标签。和QNLI相同，每个样例是单独评估的，因此模型在此任务的得分和原始任务的得分没有系统性对应关系。

***





### 【二】Code 代码

***

> 由于在工作项目中接触这块比较多，而且随着 LLM 的发展，代码生成也在飞快的迭代更新，这里梳理下



#### 【2.1】OpenAI HumanEval

***

> [HumanEval](https://huggingface.co/datasets/openai_humaneval) 来自 OpenAI，包含 164 个样本，基于 Python 和英文自然语言手动构建

***



#### 【3.2】Google MBPP

***



